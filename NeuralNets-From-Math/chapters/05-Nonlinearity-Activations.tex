\chapter{Nonlinearity and Activations}
Nonlinear activations introduce bends that let models capture curved patterns.

\begin{learningobjectives}
  \objective Describe common activations: ReLU, sigmoid, tanh.
  \objective Explain why stacking linear layers without activations stays linear.
  \objective Match activations to tasks (e.g., sigmoid for probabilities).
  \objective Understand kernel functions as an alternative approach to nonlinearity.
\end{learningobjectives}

\section{Common Activations}
ReLU: \( \mathrm{ReLU}(z)=\max(0,z) \) adds a kink at zero \cite{nair2010rectified}. Sigmoid \( \sigma(z)=1/(1+e^{-z}) \) squashes to \( (0,1) \). Tanh squashes to \( (-1,1) \).

\begin{center}
\begin{tikzpicture}
  \begin{axis}[
    width=0.45\linewidth,height=5cm,
    xmin=-4,xmax=4,ymin=-1,ymax=4,
    axis lines=middle, grid=both,
    title={ReLU},
    title style={yshift=0.5ex}]
    \addplot[domain=-4:0,samples=2,thick,color=warmstone] {0};
    \addplot[domain=0:4,samples=2,thick,color=bookpurple] {x};
  \end{axis}
\end{tikzpicture}
\hspace{1em}
\begin{tikzpicture}
  \begin{axis}[
    width=0.45\linewidth,height=5cm,
    xmin=-6,xmax=6,ymin=-0.1,ymax=1.1,
    axis lines=middle, grid=both,
    title={Sigmoid},
    title style={yshift=0.5ex}]
    \addplot[domain=-6:6,samples=400,thick,color=bookred] {1/(1+exp(-x))};
  \end{axis}
\end{tikzpicture}
\end{center}

\paragraph{Why this matters.}
Activations create nonlinearity so networks can approximate curved relationships. ReLU is simple and keeps gradients flowing for positive inputs, aiding optimisation. Sigmoid maps real numbers to \( (0,1) \), ideal for probabilities; but it can saturate (flat tails), slowing learning when inputs are very large in magnitude.

\paragraph{Bio example (sigmoid).}
The logistic (sigmoid) curve models population growth with carrying capacity and neuron firing rates as a function of membrane potential: response is low for small input, steep near threshold, and saturates at high input.

\begin{remark}
Linear \(\circ\) linear is still linear. Nonlinearity between linear steps is essential.
\end{remark}

\section{Kernel Functions}
Kernel functions offer an alternative approach to handling nonlinearity. The \emph{kernel trick} implicitly maps input data to higher-dimensional feature spaces, where linear separation becomes possible without explicitly computing the transformed features.

\begin{definition}
A \emph{kernel function} \( k(\vect x, \vect x') \) computes the inner product of feature vectors in a transformed space: \( k(\vect x, \vect x') = \langle \phi(\vect x), \phi(\vect x') \rangle \), where \( \phi \) maps inputs to a higher-dimensional space. Common kernels include the polynomial kernel and the radial basis function (RBF) kernel.
\end{definition}

\begin{example}
For data that is not linearly separable in 2D (e.g., points arranged in concentric circles), a kernel can map them to 3D where a plane can separate them. The kernel trick allows this transformation without explicitly computing the 3D coordinates, making it computationally efficient.
\end{example}

\begin{remark}
Neural networks use \emph{explicit activations} instead of kernels. Unlike kernels, which provide a fixed transformation, activations are learned during trainingâ€”each layer's weights and activations adapt to the data. This flexibility allows networks to discover task-specific nonlinear transformations rather than relying on predefined kernel functions. Additionally, activations enable deep, composable architectures that can learn hierarchical features.
\end{remark}

\section{Exercises}
\begin{exercisebox}[easy]
Sketch and compare \( \tanh(z) \) and leaky ReLU \( \mathrm{LReLU}_\alpha(z)=\max(\alpha z, z) \) with \(\alpha=0.1\). Where are they most/least sensitive, and what ranges do their outputs take?
\end{exercisebox}

\begin{hint}
ReLU is flat for \( z<0 \) and slope 1 for \( z>0 \). Sigmoid is steepest near 0, flat in tails.
\end{hint}
