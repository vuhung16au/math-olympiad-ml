\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{enumitem}

\geometry{margin=1in}

\title{From Gaussian Integral to Differentiable Programming:\\
The Jacobian Matrix and Its Applications}
\author{Nguyen Vu Hung}
\date{\today}

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\begin{document}

\maketitle

\begin{abstract}
This document explores the fundamental role of the Jacobian matrix in mathematics and its critical applications in deep learning. We begin with the classical Gaussian integral problem, demonstrating how the Jacobian transformation enables elegant solutions to otherwise intractable integrals. We then trace the evolution of this concept from multivariable calculus to modern machine learning, showing how the Jacobian serves as the mathematical backbone of backpropagation, optimisation algorithms, and neural network training. Through this journey, we connect classical mathematical theory to cutting-edge deep learning techniques.
\end{abstract}

\section{Overview}

The Jacobian matrix is one of the most important mathematical tools bridging classical calculus and modern machine learning. Named after the German mathematician Carl Gustav Jacob Jacobi (1804--1851), who made fundamental contributions to analysis and mechanics, the Jacobian was originally developed for solving multivariable calculus problems, particularly in the context of coordinate transformations and change of variables in integrals. The Jacobian has found profound applications in deep learning, where it enables efficient computation of gradients through backpropagation.

This document presents a comprehensive exploration of the Jacobian matrix, starting from its foundational role in solving the Gaussian integral, through its theoretical development in multivariable calculus, and culminating in its practical applications in neural networks and optimisation algorithms.

\section{Problem 1: The Gaussian Integral}


\begin{problem}[Gaussian Integral (Euler-Poisson Integral)]
Evaluate the integral:
\begin{equation}
I = \int_{-\infty}^{\infty} e^{-x^2} \, dx
\end{equation}
\end{problem}

This integral, also known as the Euler-Poisson integral, is fundamental in probability theory, statistics, and physics. Despite its simple appearance, it cannot be evaluated using elementary antiderivatives.

The solution that follows is verbose and lengthy, 
but it is well worth working through carefully, 
as it illustrates fundamental techniques that generalise to modern applications.


\subsection{Solution to Problem 1}

The standard approach to solving this integral involves a clever trick: squaring the integral and converting it to a double integral.

\subsubsection{Step 1: Squaring the Integral}

We consider the square of the integral:
\begin{equation}
I^2 = \left(\int_{-\infty}^{\infty} e^{-x^2} \, dx\right)^2 = \int_{-\infty}^{\infty} e^{-x^2} \, dx \cdot \int_{-\infty}^{\infty} e^{-y^2} \, dy
\end{equation}

\subsubsection{Step 2: Converting to a Double Integral}

Since the two integrals are independent, we can combine them into a double integral over the entire $xy$-plane:
\begin{equation}
I^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2 + y^2)} \, dx \, dy
\end{equation}

\subsubsection{Step 3: Direct Evaluation Attempt}

At this point, we have a double integral that is still difficult to evaluate directly in Cartesian coordinates. The integrand $e^{-(x^2 + y^2)}$ suggests that polar coordinates might be more suitable, as $x^2 + y^2 = r^2$ in polar coordinates.

\subsection{Solution Using Jacobian Transformation}

The key insight is to transform the integral from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$ using the Jacobian transformation.

\subsubsection{Coordinate Transformation}

The transformation from Cartesian to polar coordinates is:
\begin{align}
x &= r \cos \theta \\
y &= r \sin \theta
\end{align}
where $r \geq 0$ and $0 \leq \theta \leq 2\pi$.

\subsubsection{Region of Integration}

The region of integration (the entire $xy$-plane) transforms to:
\begin{align}
0 &\leq r < \infty \\
0 &\leq \theta \leq 2\pi
\end{align}

\subsubsection{The Jacobian Determinant}

The crucial step is replacing the differential area element $dx \, dy$ with the appropriate expression in polar coordinates. This requires the Jacobian determinant of the transformation, which we will compute in the next section. For now, we state that:
\begin{equation}
dx \, dy = r \, dr \, d\theta
\end{equation}
where the factor $r$ is the absolute value of the Jacobian determinant.

\subsubsection{Transformed Integral}

The double integral becomes:
\begin{equation}
I^2 = \int_{0}^{2\pi} \int_{0}^{\infty} e^{-r^2} \cdot r \, dr \, d\theta
\end{equation}

\subsubsection{Solving the Polar Integral}

We can now separate the integrals:
\begin{equation}
I^2 = \int_{0}^{2\pi} d\theta \cdot \int_{0}^{\infty} r e^{-r^2} \, dr
\end{equation}

The $\theta$ integral is straightforward:
\begin{equation}
\int_{0}^{2\pi} d\theta = 2\pi
\end{equation}

For the $r$ integral, we use the substitution $u = r^2$, so $du = 2r \, dr$, or $r \, dr = \frac{du}{2}$:
\begin{align}
\int_{0}^{\infty} r e^{-r^2} \, dr &= \int_{0}^{\infty} e^{-u} \cdot \frac{du}{2} \\
&= \frac{1}{2} \int_{0}^{\infty} e^{-u} \, du \\
&= \frac{1}{2} \left[-e^{-u}\right]_{0}^{\infty} \\
&= \frac{1}{2} (0 - (-1)) = \frac{1}{2}
\end{align}

\subsubsection{Final Result}

Substituting back:
\begin{equation}
I^2 = 2\pi \cdot \frac{1}{2} = \pi
\end{equation}

Taking the positive square root (since the integrand is always positive):
\begin{equation}
I = \int_{-\infty}^{\infty} e^{-x^2} \, dx = \sqrt{\pi}
\end{equation}

\subsection{What We Learned from Problem 1}

The solution to the Gaussian integral demonstrates several important concepts that are central to this document:

\begin{itemize}
\item \textbf{The Power of Coordinate Transformation}: The integral was intractable in Cartesian coordinates but became solvable after transforming to polar coordinates. This illustrates how choosing the right coordinate system can simplify complex problems.

\item \textbf{The Role of the Jacobian Determinant}: The key step was recognising that $dx \, dy = r \, dr \, d\theta$, where the factor $r$ is the Jacobian determinant of the transformation. This correction factor accounts for how the coordinate transformation stretches or compresses the differential area element.

\item \textbf{Connection to Multivariable Calculus}: This problem showcases the fundamental principle that when changing variables in multiple integrals, we must account for how the transformation affects the differential elements---a concept that the Jacobian matrix generalises to higher dimensions and more complex transformations.

\item \textbf{Bridge to Modern Applications}: While this classical problem uses the Jacobian for coordinate transformations in integration, the same mathematical framework---computing how transformations affect differential elements---underlies modern deep learning, where the Jacobian matrix describes how neural network layers transform input gradients during backpropagation.
\end{itemize}

This elegant solution sets the stage for understanding how the Jacobian matrix, originally developed for such classical problems, has become indispensable in modern machine learning and optimisation.

\section{The Jacobian Determinant in Multivariable Calculus}

\subsection{Definition and Explanation}

\begin{definition}[Jacobian Matrix]
For a transformation from variables $(u, v, \ldots)$ to $(x, y, \ldots)$, where $x = x(u, v, \ldots)$ and $y = y(u, v, \ldots)$, the \textbf{Jacobian matrix} $\mathbf{J}_M$ is the matrix of all first-order partial derivatives:
\begin{equation}
\mathbf{J}_M = \begin{bmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \cdots \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
\end{equation}
\end{definition}

\begin{definition}[Jacobian Determinant]
The \textbf{Jacobian determinant}, denoted $J$ or $\det(\mathbf{J}_M)$, is the determinant of the Jacobian matrix. For a transformation from $(u, v)$ to $(x, y)$:
\begin{equation}
J = \det(\mathbf{J}_M) = \begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix} = \frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial x}{\partial v} \frac{\partial y}{\partial u}
\end{equation}
\end{definition}

The Jacobian determinant acts as a scaling factor or correction factor needed when transforming a small differential volume/area element from one coordinate system to another. In a double integral:
\begin{equation}
\iint_R f(x, y) \, dx \, dy = \iint_{R'} f(x(u, v), y(u, v)) \cdot |J| \, du \, dv
\end{equation}
where $|J|$ is the absolute value of the Jacobian determinant.

\subsection{Application: Cartesian to Polar Coordinates}

In the Gaussian integral example, we transformed from Cartesian $(x, y)$ to polar $(r, \theta)$ coordinates.

\subsubsection{Transformation Equations}
\begin{align}
x &= r \cos \theta \\
y &= r \sin \theta
\end{align}

\subsubsection{Calculating Partial Derivatives}
\begin{align}
\frac{\partial x}{\partial r} &= \cos \theta, \quad \frac{\partial x}{\partial \theta} = -r \sin \theta \\
\frac{\partial y}{\partial r} &= \sin \theta, \quad \frac{\partial y}{\partial \theta} = r \cos \theta
\end{align}

\subsubsection{Calculating the Determinant}
\begin{align}
J &= \begin{vmatrix}
\cos \theta & -r \sin \theta \\
\sin \theta & r \cos \theta
\end{vmatrix} \\
&= \cos \theta \cdot r \cos \theta - (-r \sin \theta) \cdot \sin \theta \\
&= r \cos^2 \theta + r \sin^2 \theta \\
&= r (\cos^2 \theta + \sin^2 \theta) \\
&= r
\end{align}

\subsubsection{Result}

Since $r \geq 0$, the absolute value $|J| = r$. Therefore, the differential area element $dx \, dy$ is replaced by $r \, dr \, d\theta$, as used in the Gaussian integral calculation.

\section{Analogy to Single-Variable Calculus}

The Jacobian determinant generalises the concept of the derivative in single-variable calculus to multivariable functions.

In single-variable calculus, when we perform a change of variables $x = g(u)$, the differential transforms as:
\begin{equation}
dx = g'(u) \, du = \frac{dx}{du} \, du
\end{equation}

The factor $g'(u) = \frac{dx}{du}$ is the one-dimensional analog of the Jacobian determinant. In the multivariable case, the Jacobian determinant $J$ plays the same role, accounting for how the transformation stretches or compresses the differential area/volume element.

For a single-variable integral:
\begin{equation}
\int_a^b f(x) \, dx = \int_{g^{-1}(a)}^{g^{-1}(b)} f(g(u)) \cdot |g'(u)| \, du
\end{equation}

This is directly analogous to the multivariable change of variables formula using the Jacobian.

\section{Special Case: Linear Transformations}

\subsection{Jacobian of a Linear Transformation}

A linear transformation $\mathbf{T}$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is defined by an $m \times n$ matrix $\mathbf{A}$:
\begin{equation}
\mathbf{T}(\mathbf{x}) = \mathbf{A} \mathbf{x}
\end{equation}

Written component-wise:
\begin{equation}
t_i = \sum_{j=1}^n a_{ij} x_j, \quad i = 1, 2, \ldots, m
\end{equation}

\subsection{Computing the Jacobian}

The Jacobian matrix $\mathbf{J}_{\mathbf{T}}$ is the matrix of all first-order partial derivatives:
\begin{equation}
\mathbf{J}_{\mathbf{T}} = \begin{bmatrix}
\frac{\partial t_1}{\partial x_1} & \frac{\partial t_1}{\partial x_2} & \cdots & \frac{\partial t_1}{\partial x_n} \\
\frac{\partial t_2}{\partial x_1} & \frac{\partial t_2}{\partial x_2} & \cdots & \frac{\partial t_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial t_m}{\partial x_1} & \frac{\partial t_m}{\partial x_2} & \cdots & \frac{\partial t_m}{\partial x_n}
\end{bmatrix}
\end{equation}

For a linear function:
\begin{equation}
\frac{\partial t_i}{\partial x_j} = a_{ij}
\end{equation}

Therefore:
\begin{equation}
\mathbf{J}_{\mathbf{T}}(\mathbf{x}) = \mathbf{A}
\end{equation}

\subsection{Key Properties}

\begin{enumerate}
\item \textbf{Constant Jacobian}: The Jacobian matrix of a linear transformation is constant and equal to the transformation matrix $\mathbf{A}$ itself.
\item \textbf{Uniform Scaling}: Because the transformation is linear, the "stretching" or "distortion" it imposes on the input space is uniform everywhere.
\item \textbf{Best Linear Approximation}: For a function that is already linear, the best linear approximation is the function itself, so the Jacobian is exactly the matrix that defines the function.
\end{enumerate}

\subsection{Change of Variables for Linear Transformations}

If a coordinate transformation is linear (e.g., a simple scaling or rotation), the Jacobian determinant is just the determinant of the transformation matrix, and it gives the uniform scaling factor for the area or volume element.

\section{Best Linear Approximation Using Jacobian}

The Jacobian matrix represents the best linear approximation of a function near a point. For a differentiable function $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$, the linear approximation near a point $\mathbf{x}_0$ is:
\begin{equation}
\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{x}_0) + \mathbf{J}_{\mathbf{f}}(\mathbf{x}_0)(\mathbf{x} - \mathbf{x}_0)
\end{equation}

where $\mathbf{J}_{\mathbf{f}}(\mathbf{x}_0)$ is the Jacobian matrix evaluated at $\mathbf{x}_0$.

This is the multivariable generalisation of the tangent line approximation in single-variable calculus:
\begin{equation}
f(x) \approx f(x_0) + f'(x_0)(x - x_0)
\end{equation}

\section{Jacobian's Role in Optimisation}

\subsection{Generalising the First Derivative}

In single-variable calculus, an extremum (minimum or maximum) is found where the first derivative is zero: $f'(x) = 0$.

For scalar-valued functions (loss functions) $L(\mathbf{w})$ with respect to a vector of parameters $\mathbf{w}$, the Jacobian specialises to the gradient vector $\nabla L$. The Jacobian of a scalar function $\mathbf{T}: \mathbb{R}^n \to \mathbb{R}^1$ is a $1 \times n$ matrix (a row vector), which is the transpose of the standard gradient column vector:
\begin{equation}
\nabla L(\mathbf{w}) = \begin{bmatrix}
\frac{\partial L}{\partial w_1} & \frac{\partial L}{\partial w_2} & \cdots & \frac{\partial L}{\partial w_n}
\end{bmatrix}^T
\end{equation}

To find critical points, we set:
\begin{equation}
\nabla L(\mathbf{w}) = \mathbf{0}
\end{equation}

\subsection{Jacobian and Gradient Descent}

The Jacobian (in the form of the gradient) is the core of Gradient Descent (an iterative optimisation algorithm that moves in the direction of the negative gradient to minimise a function) and its variants: SGD (Stochastic Gradient Descent, which uses a random subset of data at each iteration, making it faster and more memory-efficient for large datasets) and Adam (Adaptive Moment Estimation, which combines momentum and adaptive learning rates for each parameter, often providing faster convergence and better performance on sparse gradients), which are fundamental algorithms in machine learning.

The gradient vector $\nabla L(\mathbf{w})$ points in the direction of the steepest ascent. Optimisation algorithms use the negative gradient $-\nabla L$ to determine the direction of the next step, ensuring movement toward a local minimum:
\begin{equation}
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla L(\mathbf{w}^{(t)})
\end{equation}
where $\eta$ is the learning rate (a hyperparameter that controls the step size in the direction of the negative gradient; a larger learning rate takes bigger steps but may overshoot the minimum, while a smaller learning rate takes smaller, more cautious steps but may converge slowly).

\subsection{Nonlinear Least Squares}

In Nonlinear Least Squares (NLS) problems, we minimise the sum of squares of a vector-valued function $\mathbf{F}(\mathbf{x})$:
\begin{equation}
\min_{\mathbf{x}} \sum_{i=1}^m [f_i(\mathbf{x})]^2
\end{equation}

Here, $\mathbf{F}(\mathbf{x}): \mathbb{R}^n \to \mathbb{R}^m$, 
and the Jacobian matrix $\mathbf{J}_{\mathbf{F}}$ of $\mathbf{F}$ is explicitly calculated. 
Algorithms like the Gauss-Newton method (an iterative optimisation algorithm that approximates the Hessian using the Jacobian, avoiding expensive second-derivative computations) or the Levenberg-Marquardt algorithm (a damped version of Gauss-Newton that combines the benefits of gradient descent and Gauss-Newton, particularly robust for ill-conditioned problems) use $\mathbf{J}_{\mathbf{F}}$ 
to approximate the Hessian matrix (the matrix of second-order partial derivatives, which captures the curvature of the function):
\begin{equation}
\mathbf{H} \approx \mathbf{J}_{\mathbf{F}}^T \mathbf{J}_{\mathbf{F}}
\end{equation}

Providing the analytical Jacobian to the solver significantly improves efficiency and speed of convergence compared to finite-difference approximations.

\subsection{Multi-Objective Optimisation}

The Jacobian is essential for problems where we simultaneously optimise multiple objective functions $\mathbf{F}(\mathbf{x}) = (f_1(\mathbf{x}), f_2(\mathbf{x}), \ldots, f_m(\mathbf{x}))$. The Jacobian $\mathbf{J}_{\mathbf{F}}$ is composed of the gradients of all $m$ objective functions, stacked as rows:
\begin{equation}
\mathbf{J}_{\mathbf{F}} = \begin{bmatrix}
\nabla f_1(\mathbf{x})^T \\
\nabla f_2(\mathbf{x})^T \\
\vdots \\
\nabla f_m(\mathbf{x})^T
\end{bmatrix}
\end{equation}

\section{Jacobian Matrix in Deep Learning}

The Jacobian matrix is fundamental to deep learning, primarily serving as the mathematical backbone for backpropagation, the algorithm used to train neural networks.

A deep neural network is a vector-valued composite function that maps an input vector (like an image or a set of features) to an output vector (like classification probabilities). The Jacobian provides a way to calculate all the necessary first-order partial derivatives of this complex, multi-layered function.

\subsection{The Jacobian Matrix Definition}

The Jacobian matrix $\mathbf{J}$ collects all the first-order partial derivatives of a function that has multiple inputs and multiple outputs. If $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$, the Jacobian is an $m \times n$ matrix:
\begin{equation}
\mathbf{J} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
\end{equation}

\subsection{Backpropagation}

\subsubsection{Chain Rule}

Training a neural network requires Gradient Descent, which minimises a loss function $L$ by adjusting the network's weights $\mathbf{W}$. This requires calculating the gradient $\nabla_{\mathbf{W}} L$.

Since the loss $L$ depends on the network's final output $\mathbf{y}$, and $\mathbf{y}$ depends on the weights $\mathbf{W}$ across many layers, the gradient must be computed using the multivariate Chain Rule.

\subsubsection{Layer-wise Derivatives}

For each layer transformation $f^{(l)}$ with input $\mathbf{x}^{(l)}$ and output $\mathbf{x}^{(l+1)}$, the derivative $\frac{\partial \mathbf{x}^{(l+1)}}{\partial \mathbf{x}^{(l)}}$ is the Jacobian matrix of that layer.

\subsubsection{Propagation}

Backpropagation efficiently uses matrix multiplication of these Jacobian matrices, propagating the error signal (gradient) backward through the network layers to update the weights. This allows the network to efficiently compute the final gradient:
\begin{equation}
\nabla_{\mathbf{W}} L = \mathbf{J}^{(L)} \mathbf{J}^{(L-1)} \cdots \mathbf{J}^{(1)}
\end{equation}
where each $\mathbf{J}^{(l)}$ is the Jacobian matrix of layer $l$.

\subsection{Neural Networks}

In a neural network, each layer performs a transformation:
\begin{equation}
\mathbf{x}^{(l+1)} = \sigma(\mathbf{W}^{(l)} \mathbf{x}^{(l)} + \mathbf{b}^{(l)})
\end{equation}
where $\sigma$ is an activation function, $\mathbf{W}^{(l)}$ is the weight matrix, and $\mathbf{b}^{(l)}$ is the bias vector.

The Jacobian of this transformation with respect to the input $\mathbf{x}^{(l)}$ is:
\begin{equation}
\mathbf{J}^{(l)} = \frac{\partial \mathbf{x}^{(l+1)}}{\partial \mathbf{x}^{(l)}} = \text{diag}(\sigma'(\mathbf{z}^{(l)})) \mathbf{W}^{(l)}
\end{equation}
where $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{x}^{(l)} + \mathbf{b}^{(l)}$ and $\sigma'$ is the derivative of the activation function.

\subsection{Analysis and Stability}

Beyond training, the Jacobian is used to analyse the behaviour of trained models:

\begin{itemize}
\item \textbf{Sensitivity Analysis}: The Jacobian of the output with respect to the input $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ reveals how sensitive the model's output is to small changes in the input data. Large elements in this Jacobian indicate that the network output is very volatile, which often corresponds to a lack of robustness.

\item \textbf{Adversarial Attacks}: Adversarial examples (inputs slightly modified to trick the network) are often constructed using information from the Jacobian $\frac{\partial L}{\partial \mathbf{x}}$ to find the direction of input change that most rapidly increases the loss.

\item \textbf{Gradient Stability}: The spectral norm (or largest singular value) of the Jacobian of the layer-wise transformation is key to understanding and mitigating the vanishing/exploding gradient problem. Initialisation schemes like Xavier or Kaiming aim to keep the norm of these Jacobians close to 1 to ensure effective training.
\end{itemize}

\section{Advanced Optimisations}

\subsection{Newton's Method}

Newton's method is a second-order optimisation method that uses the Hessian matrix (the matrix of second-order partial derivatives). The Hessian can be approximated using the Jacobian:
\begin{equation}
\mathbf{H} \approx \mathbf{J}^T \mathbf{J}
\end{equation}

Newton's method update rule is:
\begin{equation}
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \mathbf{H}^{-1} \nabla L(\mathbf{x}^{(t)})
\end{equation}

While more computationally expensive than gradient descent, Newton's method can converge much faster, especially near the optimum.

\subsection{Jacobian-Enhanced Neural Networks (JENN)}

Jacobian-Enhanced Neural Networks (JENN) are specialised networks that explicitly include terms in their loss function to ensure the network not only fits the target function values $\mathbf{F}(\mathbf{x})$ but also accurately predicts the Jacobian matrix $\mathbf{J}(\mathbf{x})$. This leads to higher accuracy with less training data.

The loss function for JENN includes both function value error and Jacobian error:
\begin{equation}
L_{\text{JENN}} = L_{\text{function}} + \lambda L_{\text{Jacobian}}
\end{equation}
where $\lambda$ is a weighting parameter.

\section{Jacobian Matrix and Sigmoid Functions}

The Jacobian matrix is applied to Sigmoid functions when they are used as activation functions in neural networks. The simplicity of the Sigmoid's Jacobian is one of the features that historically made it a popular choice for activation layers.

\subsection{Element-Wise Nature}

The Sigmoid function, $\sigma(x) = \frac{1}{1 + e^{-x}}$, is applied element-wise to the input vector $\mathbf{z}$ of a layer. If the input to the activation function is a vector $\mathbf{z} = (z_1, z_2, \ldots, z_n)$, the output $\mathbf{a}$ is also a vector:
\begin{equation}
\mathbf{a} = \begin{bmatrix}
\sigma(z_1) \\
\sigma(z_2) \\
\vdots \\
\sigma(z_n)
\end{bmatrix}
\end{equation}

Crucially, the output $a_i$ only depends on its corresponding input $z_i$, not on any other input $z_j$ (where $j \neq i$).

\subsection{The Diagonal Jacobian}

Because of this element-wise independence, the Jacobian matrix $\mathbf{J}$ of the Sigmoid activation function is a diagonal matrix. The off-diagonal entries (the partial derivatives of $a_i$ with respect to $z_j$, where $i \neq j$) are all zero.

\subsection{Derivative Term}

The diagonal elements of the Jacobian are the simple scalar derivative of the Sigmoid function itself:
\begin{equation}
\sigma'(x) = \sigma(x)(1 - \sigma(x))
\end{equation}

So, the Jacobian can be written as:
\begin{equation}
\mathbf{J}_{\sigma} = \text{diag}(\sigma'(\mathbf{z})) = \text{diag}(\sigma(\mathbf{z}) \odot (1 - \sigma(\mathbf{z})))
\end{equation}
where $\odot$ denotes the element-wise (Hadamard) product.

\subsection{Backpropagation Optimisation}

During backpropagation, the gradient from the subsequent layer, $\boldsymbol{\delta}_{l+1}$ (often a vector), is multiplied by the Jacobian to get the gradient for the current layer, $\boldsymbol{\delta}_{l}$:
\begin{equation}
\boldsymbol{\delta}_{l} = \mathbf{J}_{\sigma}^T \boldsymbol{\delta}_{l+1}
\end{equation}

Since $\mathbf{J}_{\sigma}$ is diagonal, this matrix-vector multiplication simplifies to a much faster element-wise multiplication in practice:
\begin{equation}
\boldsymbol{\delta}_{l} = \sigma'(\mathbf{z}) \odot \boldsymbol{\delta}_{l+1}
\end{equation}

This optimisation is a key reason why element-wise activation functions like Sigmoid, ReLU, and Tanh are computationally efficient to train. Contrast this with the Softmax function, which is element-dependent, resulting in a full, non-diagonal Jacobian and requiring a more complex matrix multiplication.

\section{Jacobian Descent (JD)}

Jacobian Descent (JD) is a newer optimisation technique that uses the full Jacobian matrix to find a parameter update direction that balances all objectives in multi-objective optimisation problems. This is particularly useful when objectives conflict.

In multi-objective optimisation, we have:
\begin{equation}
\mathbf{F}(\mathbf{x}) = (f_1(\mathbf{x}), f_2(\mathbf{x}), \ldots, f_m(\mathbf{x}))
\end{equation}

The Jacobian $\mathbf{J}_{\mathbf{F}}$ is composed of the gradients of all $m$ objective functions. Jacobian Descent uses this full matrix to find update directions that prevent any single objective from degrading too much, even when objectives conflict.

The update rule for Jacobian Descent can be formulated as:
\begin{equation}
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \eta \mathbf{J}_{\mathbf{F}}^T \mathbf{d}
\end{equation}
where $\mathbf{d}$ is a direction vector that balances the objectives.

\section{Differentiable Programming}

Differentiable Programming (DP) represents a paradigm shift that generalises the principles underlying deep learning to a broader class of computational problems. While neural networks are a specific application, differentiable programming extends the use of automatic differentiation and gradient-based optimisation to arbitrary programs containing differentiable operations.

\subsection{From Neural Networks to Differentiable Programs}

The evolution from classical analysis to modern computing can be traced as:
\begin{enumerate}
\item \textbf{Classical Analysis}: The Gaussian integral problem (Section 2) required the Jacobian determinant for coordinate transformations.
\item \textbf{Multivariable Calculus}: The Jacobian matrix (Section 3) generalised derivatives to vector-valued functions.
\item \textbf{Neural Networks}: Backpropagation (Section 8) applied the Jacobian through the chain rule for training.
\item \textbf{Differentiable Programming}: The Jacobian becomes the universal tool for making arbitrary computational pipelines optimisable.
\end{enumerate}

\subsection{The Jacobian as Universal Differentiator}

In differentiable programming, any computation that can be expressed as a composition of differentiable operations has a well-defined Jacobian. This includes:

\begin{itemize}
\item \textbf{Physics Simulations}: Differentiable physics engines use Jacobians to optimise physical parameters (mass, friction) by backpropagating through simulation steps.
\item \textbf{Graphics and Rendering}: Differentiable renderers compute gradients of rendered images with respect to scene parameters (camera position, object geometry, lighting).
\item \textbf{Probabilistic Inference}: Automatic differentiation enables gradient-based inference in complex probabilistic models.
\item \textbf{Scientific Computing}: Solving differential equations with learned components requires Jacobians through both the solver and the learned functions.
\end{itemize}

\subsection{Mathematical Framework}

A differentiable program can be viewed as a composition of functions:
\begin{equation}
\mathbf{y} = f_L \circ f_{L-1} \circ \cdots \circ f_2 \circ f_1(\mathbf{x}, \boldsymbol{\theta})
\end{equation}
where each $f_i$ is differentiable with respect to its inputs and parameters $\boldsymbol{\theta}$.

The gradient of the output with respect to parameters is computed via the chain rule:
\begin{equation}
\frac{\partial \mathbf{y}}{\partial \boldsymbol{\theta}} = \frac{\partial f_L}{\partial f_{L-1}} \cdot \frac{\partial f_{L-1}}{\partial f_{L-2}} \cdots \frac{\partial f_2}{\partial f_1} \cdot \frac{\partial f_1}{\partial \boldsymbol{\theta}}
\end{equation}

Each term $\frac{\partial f_i}{\partial f_{i-1}}$ is a Jacobian matrix, making the entire computation a product of Jacobians---exactly as in neural network backpropagation, but applied to arbitrary computational graphs.

\subsection{Automatic Differentiation Systems}

Modern automatic differentiation (autodiff) frameworks like TensorFlow, PyTorch, and JAX implement differentiable programming by:

\begin{enumerate}
\item \textbf{Building Computational Graphs}: Recording operations as a directed acyclic graph (DAG).
\item \textbf{Forward Pass}: Computing outputs by traversing the graph forward.
\item \textbf{Backward Pass}: Computing gradients by traversing backward, multiplying Jacobian matrices at each node.
\item \textbf{Jacobian-Vector Products}: For efficiency, most systems compute Jacobian-vector products $\mathbf{J} \mathbf{v}$ rather than full Jacobian matrices.
\end{enumerate}

\subsection{Key Differences from Traditional Programming}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{Traditional} & \textbf{Differentiable} \\
\hline
Operations & Any & Must be differentiable \\
Control flow & Arbitrary branches & Limited (or differentiable) \\
Optimisation & Manual tuning & Gradient-based \\
Debugging & Output correctness & Gradient correctness \\
\hline
\end{tabular}
\end{center}

\subsection{Applications Beyond Deep Learning}

\subsubsection{Differentiable Physics}

Physics simulations traditionally solve forward problems (given parameters, predict behaviour). Differentiable physics solves inverse problems: given desired behaviour, find parameters.

Example: Robot control optimisation. The Jacobian of the simulator output (robot trajectory) with respect to control inputs enables gradient-based policy search.

\subsubsection{Differentiable Rendering}

Rendering converts 3D scene parameters to 2D images. Differentiable rendering computes:
\begin{equation}
\frac{\partial \text{Image}}{\partial \text{Scene Parameters}}
\end{equation}

This Jacobian enables fitting 3D models to 2D observations, enabling applications like 3D reconstruction and inverse graphics.

\subsubsection{Program Synthesis}

Differentiable interpreters allow learning programs through gradient descent. The Jacobian of program output with respect to program parameters (weights, structure) makes traditional discrete program spaces continuous and optimisable.

\subsection{Challenges and Research Directions}

\begin{enumerate}
\item \textbf{Discrete Operations}: Many algorithms involve non-differentiable operations (argmax, sorting). Research focuses on differentiable relaxations.
\item \textbf{Memory Efficiency}: Storing intermediate values for backpropagation can be memory-intensive. Gradient checkpointing and reversible architectures address this.
\item \textbf{Numerical Stability}: Long chains of Jacobian multiplications can lead to numerical issues (vanishing/exploding gradients).
\item \textbf{Higher-Order Derivatives}: Some applications require Hessians (second-order) or beyond, requiring nested autodiff.
\end{enumerate}

\subsection{Connection to This Document's Themes}

Differentiable programming demonstrates how the Jacobian matrix---first encountered in the 18th-century Gaussian integral problem---has become the mathematical foundation for making entire computer programs ``learnable'':

\begin{itemize}
\item The \textbf{coordinate transformation} insight from Section 2 generalises to transforming any computational operation.
\item The \textbf{chain rule} from multivariable calculus (Section 3) becomes the core algorithm for autodiff systems.
\item The \textbf{backpropagation} technique from neural networks (Section 8) extends to arbitrary computational graphs.
\item The \textbf{optimisation methods} (Sections 6--10) apply unchanged to differentiable programs.
\end{itemize}

Differentiable programming represents the ultimate generalisation: not just differentiable functions, but differentiable computation itself, all enabled by the humble Jacobian matrix.

\section{Further Works and Open Problems}

Several open problems and areas of active research remain:

\begin{enumerate}
\item \textbf{Computational Efficiency}: While automatic differentiation makes Jacobian computation feasible, computing full Jacobians for very large networks remains computationally expensive. Research continues into more efficient approximations and sparse Jacobian representations.

\item \textbf{Second-Order Methods}: Extending Jacobian-based methods to incorporate second-order information (Hessian) efficiently for large-scale deep learning remains challenging.

\item \textbf{Adversarial Robustness}: Understanding how to use Jacobian information to build more robust networks that are resistant to adversarial attacks is an active area of research.

\item \textbf{Neural ODEs}: The connection between continuous-time neural networks (Neural ODEs) and the Jacobian provides new perspectives on network dynamics and training stability.

\item \textbf{Implicit Neural Representations}: Using Jacobian constraints in loss functions for implicit neural representations (INRs) to ensure smoothness and differentiability.

\item \textbf{Multi-Objective Learning}: Developing more sophisticated Jacobian-based methods for multi-objective optimisation in deep learning, particularly for tasks with competing objectives.

\item \textbf{Differentiable Programming}: Extending Jacobian-based techniques to broader computational pipelines beyond neural networks, including differentiable physics, rendering, and program synthesis.
\end{enumerate}

\section{Conclusions}

The Jacobian matrix serves as a fundamental bridge between classical mathematics and modern machine learning. From its origins in solving the Gaussian integral through coordinate transformations, to its central role in backpropagation and neural network training, the Jacobian demonstrates the deep connections between mathematical theory and practical applications.

Key takeaways:

\begin{itemize}
\item The Jacobian determinant enables elegant solutions to otherwise intractable integrals through coordinate transformations.

\item The Jacobian matrix generalises the concept of the derivative to multivariable and vector-valued functions.

\item In deep learning, the Jacobian is the mathematical foundation of backpropagation, enabling efficient gradient computation through the chain rule.

\item The structure of the Jacobian (e.g., diagonal for element-wise activations) provides computational optimisations that make modern deep learning feasible.

\item Advanced techniques like JENN and Jacobian Descent demonstrate how explicit use of Jacobian information can improve model performance and training efficiency.

\item Differentiable programming extends the Jacobian framework beyond neural networks to arbitrary computational pipelines, enabling gradient-based optimisation of physics simulations, rendering engines, and program synthesis.

\item The Jacobian continues to inspire new research directions in optimisation, robustness, and neural network design.
\end{itemize}

As deep learning continues to evolve, the Jacobian matrix remains a cornerstone of the mathematical framework that makes these advances possible. Understanding its theory and applications is essential for both theoretical research and practical implementation of modern machine learning systems.

\vspace{2cm}

\appendix

\section{Example: Finding Extrema of a Function}

The Jacobian matrix is primarily used in multivariable calculus for coordinate transformations and for finding the derivative of a vector-valued function. While the Jacobian matrix itself is not directly used to find the maximum or minimum of a function, its first-order counterpart, the gradient vector (which is the transpose of the Jacobian matrix of a scalar function), is essential in this process.

The search for maximum or minimum values (extrema) of a function is better exemplified by using the gradient vector (first derivative test) and the Hessian matrix (second derivative test). Let us work through a concrete example.

\subsection{Problem Statement}

Find the critical points and determine the nature of the extrema (maximum, minimum, or saddle point) for the function:
\begin{equation}
f(x, y) = x^2 + 4y^2 - 11x + 8y - 16
\end{equation}

\subsection{Gradient Calculation (Related to the Jacobian)}

The function $f(x, y)$ is a scalar function of two variables, so its first derivative is the gradient vector, $\nabla f(x, y)$, which is the transpose of the Jacobian matrix of $f$.

The Jacobian of a scalar function $f(\mathbf{x}): \mathbb{R}^n \to \mathbb{R}$ is a $1 \times n$ matrix of its partial derivatives:
\begin{equation}
\mathbf{J}_f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \cdots & \frac{\partial f}{\partial x_n} \end{pmatrix}
\end{equation}

For our function $f(x, y)$, the gradient (and the transpose of the Jacobian) is:
\begin{equation}
\nabla f(x, y) = \mathbf{J}_f(x,y)^T = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix}
\end{equation}

Computing the partial derivatives:
\begin{align}
\frac{\partial f}{\partial x} &= \frac{\partial}{\partial x} (x^2 + 4y^2 - 11x + 8y - 16) = 2x - 11 \\
\frac{\partial f}{\partial y} &= \frac{\partial}{\partial y} (x^2 + 4y^2 - 11x + 8y - 16) = 8y + 8
\end{align}

Therefore, the gradient vector is:
\begin{equation}
\nabla f(x, y) = \begin{pmatrix} 2x - 11 \\ 8y + 8 \end{pmatrix}
\end{equation}

\subsection{Finding Critical Points (First Derivative Test)}

To find critical points, we set the gradient equal to the zero vector:
\begin{equation}
\nabla f(x, y) = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{equation}

This gives us a system of equations:
\begin{align}
2x - 11 &= 0 \implies 2x = 11 \implies x = \frac{11}{2} \\
8y + 8 &= 0 \implies 8y = -8 \implies y = -1
\end{align}

The only critical point is $\left(\frac{11}{2}, -1\right)$.

\subsection{The Hessian Matrix (Second Derivative Test)}

The Hessian matrix, $\mathbf{H}$, is used to determine if a critical point is a local maximum, local minimum, or a saddle point. The Hessian is the matrix of second-order partial derivatives, which can be thought of as the Jacobian of the gradient vector (or the Jacobian of the Jacobian's transpose).

For a function $f(x, y)$, the Hessian matrix is:
\begin{equation}
\mathbf{H}(x, y) = \begin{pmatrix} 
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\ 
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} 
\end{pmatrix}
\end{equation}

Computing the second-order partial derivatives:
\begin{align}
\frac{\partial^2 f}{\partial x^2} &= \frac{\partial}{\partial x} (2x - 11) = 2 \\
\frac{\partial^2 f}{\partial y^2} &= \frac{\partial}{\partial y} (8y + 8) = 8 \\
\frac{\partial^2 f}{\partial x \partial y} &= \frac{\partial}{\partial x} (8y + 8) = 0 \\
\frac{\partial^2 f}{\partial y \partial x} &= \frac{\partial}{\partial y} (2x - 11) = 0
\end{align}

Therefore, the Hessian matrix is:
\begin{equation}
\mathbf{H}(x, y) = \begin{pmatrix} 2 & 0 \\ 0 & 8 \end{pmatrix}
\end{equation}

Since the Hessian is constant, the second derivative test results will be the same for all points, including the critical point $\left(\frac{11}{2}, -1\right)$.

\subsection{Determining the Extrema}

We use the determinant of the Hessian, $\det(\mathbf{H})$, and the element $H_{11}$ (or $\frac{\partial^2 f}{\partial x^2}$) to classify the critical point.

First, calculate the determinant:
\begin{equation}
\det(\mathbf{H}) = (2)(8) - (0)(0) = 16
\end{equation}

Now apply the second derivative test at the critical point $\left(\frac{11}{2}, -1\right)$:

\begin{itemize}
\item If $\det(\mathbf{H}) > 0$ and $H_{11} > 0$: \textbf{Local Minimum}
\item If $\det(\mathbf{H}) > 0$ and $H_{11} < 0$: \textbf{Local Maximum}
\item If $\det(\mathbf{H}) < 0$: \textbf{Saddle Point}
\item If $\det(\mathbf{H}) = 0$: Test is inconclusive
\end{itemize}

In our case:
\begin{itemize}
\item $\det(\mathbf{H}) = 16 > 0$
\item $H_{11} = 2 > 0$
\end{itemize}

Therefore, the function has a \textbf{Local Minimum} at the point $\left(\frac{11}{2}, -1\right)$.

The value of the minimum is:
\begin{align}
f\left(\frac{11}{2}, -1\right) &= \left(\frac{11}{2}\right)^2 + 4(-1)^2 - 11\left(\frac{11}{2}\right) + 8(-1) - 16 \\
&= \frac{121}{4} + 4 - \frac{121}{2} - 8 - 16 \\
&= \frac{121}{4} + \frac{16}{4} - \frac{242}{4} - \frac{32}{4} - \frac{64}{4} \\
&= \frac{121 + 16 - 242 - 32 - 64}{4} \\
&= \frac{-201}{4}
\end{align}

This example demonstrates how the gradient (related to the Jacobian) and the Hessian matrix work together to find and classify extrema of multivariable functions, providing a concrete illustration of the optimisation principles discussed in this document.

\vspace{2cm}

\section*{Contact Information}

\begin{itemize}
\item \textbf{LinkedIn}: \url{https://www.linkedin.com/in/nguyenvuhung/}
\item \textbf{GitHub}: \url{https://github.com/vuhung16au/}
\item \textbf{Repo}: \url{https://github.com/vuhung16au/math-olympiad-ml/tree/main/JacobianMathsToDeepLearning}
\end{itemize}

\end{document}

